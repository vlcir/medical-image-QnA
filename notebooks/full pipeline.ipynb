{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch torchvision torchaudio\n!pip install -q transformers datasets accelerate Pillow huggingface_hub\n!pip install -q bert_score\n!pip install -q --no-deps peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:40:38.395722Z","iopub.execute_input":"2025-12-21T16:40:38.396412Z","iopub.status.idle":"2025-12-21T16:40:50.421639Z","shell.execute_reply.started":"2025-12-21T16:40:38.396378Z","shell.execute_reply":"2025-12-21T16:40:50.420847Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPVisionModel, AutoImageProcessor\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom huggingface_hub import snapshot_download\nimport gc\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport warnings\nfrom tqdm.auto import tqdm\nimport logging\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks import Callback\n\nwarnings.filterwarnings('ignore')\nos.environ['TRANSFORMERS_VERBOSITY'] = 'error'\nlogging.getLogger('transformers').setLevel(logging.ERROR)\n\ndef apply_lora_to_qwen(\n    qwen,\n    r=8,\n    alpha=16,\n    dropout=0.05\n):\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        inference_mode=False,\n        r=r,\n        lora_alpha=alpha,\n        lora_dropout=dropout,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"gate_proj\",\n            \"up_proj\",\n            \"down_proj\"\n        ]\n    )\n\n    qwen = get_peft_model(qwen, lora_config)\n    qwen.print_trainable_parameters()\n    return qwen\n\ndef load_qwen_frozen():\n    model_name = \"Qwen/Qwen1.5-0.5B\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    qwen = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float32,\n        device_map=\"cuda:0\",\n        load_in_8bit=False,\n        low_cpu_mem_usage=True\n    )\n    qwen.config.pad_token_id = tokenizer.pad_token_id\n    for p in qwen.parameters():\n        p.requires_grad = False\n    qwen.gradient_checkpointing_enable()\n    qwen.enable_input_require_grads()\n    return tokenizer, qwen\n\nclass VqaradDataset(Dataset):\n    def __init__(self, image_processor, split='train', hf_repo_id='flaviagiammarino/vqa-rad', max_samples=None):\n        self.image_processor = image_processor\n        self.dataset = load_dataset(hf_repo_id, split=split, streaming=False)\n        self.dataset = list(self.dataset)\n        if max_samples is not None:\n            self.dataset = self.dataset[:max_samples]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        image = item['image'].convert(\"RGB\")\n        image = image.resize((224, 224), Image.LANCZOS)\n        processed_output = self.image_processor(image, return_tensors=\"pt\")\n        if hasattr(processed_output, 'pixel_values'):\n            processed_image = processed_output.pixel_values\n        else:\n            processed_image = processed_output\n        if processed_image.dim() == 4:\n            processed_image = processed_image.squeeze(0)\n        image.close()\n        del image\n        return {\n            \"image\": processed_image,\n            \"question\": item['question'],\n            \"answer\": item['answer']\n        }\n\nclass VisionMLP(nn.Module):\n    def __init__(self, vision_encoder, encoder_output_dim, mlp_output_dim, hidden_dim=1280):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.mlp = nn.Sequential(\n            nn.Linear(encoder_output_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, mlp_output_dim),\n            nn.LayerNorm(mlp_output_dim)\n        )\n        self._init_weights()\n\n    def _init_weights(self):\n        for module in self.mlp.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight, gain=0.02)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n\n    def forward(self, pixel_values):\n        with torch.no_grad():\n            outputs = self.vision_encoder(pixel_values, output_hidden_states=False)\n            patch_tokens = outputs.last_hidden_state[:, 1:, :]\n            if torch.isnan(patch_tokens).any():\n                patch_tokens = torch.nan_to_num(patch_tokens, nan=0.0)\n        original_dtype = patch_tokens.dtype\n        patch_tokens_fp32 = patch_tokens.float()\n        mlp_output = self.mlp(patch_tokens_fp32)\n        mlp_output = torch.clamp(mlp_output, min=-10.0, max=10.0)\n        if torch.isnan(mlp_output).any():\n            mlp_output = torch.nan_to_num(mlp_output, nan=0.0)\n        return mlp_output.to(original_dtype)\n\ndef get_model(encoder_choice, mlp_output_dim, hidden_dim=1280):\n    device = \"cuda:0\"\n    model_paths = {\n        'standard_clip': 'openai/clip-vit-base-patch32',\n        'pubmedclip': 'flaviagiammarino/pubmed-clip-vit-base-patch32',\n    }\n    vision_encoder = CLIPVisionModel.from_pretrained(model_paths[encoder_choice]).to(device)\n    image_processor = AutoImageProcessor.from_pretrained(model_paths[encoder_choice])\n    encoder_output_dim = vision_encoder.config.hidden_size\n    model = VisionMLP(\n        vision_encoder=vision_encoder,\n        encoder_output_dim=encoder_output_dim,\n        mlp_output_dim=mlp_output_dim,\n        hidden_dim=hidden_dim\n    ).to(device)\n    for param in model.vision_encoder.parameters():\n        param.requires_grad = False\n    return model, image_processor\n\nclass MultiModalVQA(nn.Module):\n    def __init__(self, vision_mlp_model, qwen, tokenizer):\n        super().__init__()\n        self.vision = vision_mlp_model\n        self.qwen = qwen\n        self.tokenizer = tokenizer\n        self.device = torch.device(\"cuda:0\")\n\n    def get_base_model(self):\n        if hasattr(self.qwen, \"get_input_embeddings\"):\n            return self.qwen\n        if hasattr(self.qwen, \"base_model\"):\n            return self.qwen.base_model\n        raise RuntimeError(\"Cannot locate base language model\")\n\n    def forward(self, images, questions, answers=None):\n        device = self.device\n        dtype = self.qwen.dtype\n        patch_embeds = self.vision(images)\n        if torch.isnan(patch_embeds).any():\n            patch_embeds = torch.nan_to_num(patch_embeds, nan=0.0)\n        patch_embeds = patch_embeds.to(dtype).to(device)\n        B, P, H = patch_embeds.shape\n        q_tok = self.tokenizer(\n            questions,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=64\n        ).to(device)\n        \n\n        base_model = self.get_base_model()\n        text_embeds = base_model.get_input_embeddings()(q_tok.input_ids)\n\n        \n        if torch.isnan(text_embeds).any():\n            text_embeds = torch.nan_to_num(text_embeds, nan=0.0)\n        text_mask = q_tok.attention_mask.to(device)\n        inputs_embeds = torch.cat([patch_embeds, text_embeds], dim=1)\n        patch_mask = torch.ones((B, P), device=device, dtype=text_mask.dtype)\n        attention_mask = torch.cat([patch_mask, text_mask], dim=1)\n        labels = None\n        if answers is not None:\n            a_tok = self.tokenizer(\n                answers,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=32\n            ).to(device)\n            labels_list = []\n            for i in range(B):\n                vis_pad = torch.full((P,), -100, device=device, dtype=torch.long)\n                q_len = text_mask[i].sum().item()\n                q_pad = torch.full((q_len,), -100, device=device, dtype=torch.long)\n                ans_len = (a_tok.input_ids[i] != self.tokenizer.pad_token_id).sum().item()\n                ans_tokens = a_tok.input_ids[i, :ans_len]\n                label_seq = torch.cat([vis_pad, q_pad, ans_tokens], dim=0)\n                labels_list.append(label_seq)\n            max_len = max(len(l) for l in labels_list)\n            labels = torch.full((B, max_len), -100, device=device, dtype=torch.long)\n            for i, label_seq in enumerate(labels_list):\n                labels[i, :len(label_seq)] = label_seq\n            a_embeds = base_model.get_input_embeddings()(a_tok.input_ids).to(dtype).to(device)\n            if torch.isnan(a_embeds).any():\n                a_embeds = torch.nan_to_num(a_embeds, nan=0.0)\n            a_mask = (a_tok.input_ids != self.tokenizer.pad_token_id).long().to(device)\n            inputs_embeds = torch.cat([inputs_embeds, a_embeds], dim=1)\n            attention_mask = torch.cat([attention_mask, a_mask], dim=1)\n            if labels.size(1) != inputs_embeds.size(1):\n                if labels.size(1) > inputs_embeds.size(1):\n                    pad_len = labels.size(1) - inputs_embeds.size(1)\n                    pad_emb = torch.zeros(B, pad_len, H, device=device, dtype=dtype)\n                    inputs_embeds = torch.cat([inputs_embeds, pad_emb], dim=1)\n                    pad_mask = torch.zeros(B, pad_len, device=device, dtype=attention_mask.dtype)\n                    attention_mask = torch.cat([attention_mask, pad_mask], dim=1)\n                elif inputs_embeds.size(1) > labels.size(1):\n                    pad_len = inputs_embeds.size(1) - labels.size(1)\n                    pad_labels = torch.full((B, pad_len), -100, device=device, dtype=torch.long)\n                    labels = torch.cat([labels, pad_labels], dim=1)\n        if torch.isnan(inputs_embeds).any():\n            inputs_embeds = torch.nan_to_num(inputs_embeds, nan=0.0)\n        out = self.qwen(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        return out\n\nclass LitMultiModalVQA(pl.LightningModule):\n    def __init__(\n        self,\n        model: MultiModalVQA,\n        lr: float,\n        stage: str = \"mlp\"\n    ):\n        super().__init__()\n        self.add_module(\"model\", model)  # Register as submodule\n        self.lr = lr\n        self.stage = stage\n        self.save_hyperparameters(ignore=[\"model\"])\n\n    def forward(self, images, questions, answers=None):\n        return self.model(images, questions, answers)\n\n    def training_step(self, batch, batch_idx):\n        images = batch[\"image\"]\n        questions = batch[\"question\"]\n        answers = batch[\"answer\"]\n\n        out = self(images, questions, answers)\n        loss = out.loss\n\n        self.log(\n            \"train_loss\",\n            loss,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            batch_size=images.size(0)\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images = batch[\"image\"]\n        questions = batch[\"question\"]\n        answers = batch[\"answer\"]\n\n        out = self(images, questions, answers)\n        loss = out.loss\n\n        self.log(\n            \"val_loss\",\n            loss,\n            on_epoch=True,\n            prog_bar=True,\n            batch_size=images.size(0)\n        )\n        return loss\n\n    def configure_optimizers(self):\n        if self.stage == \"mlp\":\n            params = self.model.vision.mlp.parameters()\n        else:\n            params = self.model.qwen.parameters()\n\n        optimizer = torch.optim.AdamW(\n            params,\n            lr=self.lr,\n            weight_decay=0.01\n        )\n        return optimizer\n\ndef get_dataloader(dataset_choice, image_processor, batch_size=2, max_samples=None):\n    if dataset_choice == 'vqa_rad':\n        dataset = VqaradDataset(image_processor=image_processor, split='train', max_samples=max_samples)\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=0,\n        pin_memory=False\n    )\n\ndef save_checkpoint(model, epoch, checkpoint_dir=\"checkpoints\", stage=\"mlp\"):\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    checkpoint_path = os.path.join(checkpoint_dir, f\"{stage}_epoch_{epoch}.pt\")\n    \n    if stage == \"mlp\":\n        torch.save({\n            'epoch': epoch,\n            'vision_mlp_state_dict': model.vision.state_dict(),\n            'model_state_dict': model.state_dict(),\n        }, checkpoint_path)\n    else:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n        }, checkpoint_path)\n    \n    print(f\"Checkpoint saved: {checkpoint_path}\")\n\nclass BertF1Callback(Callback):\n    def __init__(self, test_dataloader, device=\"cuda:0\", num_examples=3):\n        super().__init__()\n        self.test_dataloader = test_dataloader\n        self.device = device\n        self.num_examples = num_examples\n\n    @torch.no_grad()\n    def on_epoch_end(self, trainer, pl_module):\n        pl_module.eval()\n        model = pl_module.model\n        preds, refs, questions = [], [], []\n\n        for batch_idx, batch in enumerate(self.test_dataloader):\n            images = batch[\"image\"].to(self.device)\n            batch_questions = batch[\"question\"]\n            batch_answers = batch[\"answer\"]\n            batch_preds = generate_answer(model, images, batch_questions, device=self.device)\n            preds.extend(batch_preds)\n            refs.extend(batch_answers)\n            questions.extend(batch_questions)\n            if len(preds) >= self.num_examples:\n                break\n\n        print(\"\\nSample predictions (Epoch {}):\".format(trainer.current_epoch + 1))\n        for i in range(min(self.num_examples, len(preds))):\n            print(f\"\\n  Example {i + 1}:\")\n            print(f\"    Question:   {questions[i]}\")\n            print(f\"    Predicted:  {preds[i]}\")\n            print(f\"    Reference:  {refs[i]}\")\n\n        pl_module.train()\n\nclass CheckpointEveryEpoch(Callback):\n    def __init__(self, stage):\n        super().__init__()\n        self.stage = stage\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        save_checkpoint(pl_module.model, trainer.current_epoch + 1, stage=self.stage)\n\n@torch.no_grad()\ndef generate_answer(model, images, questions, max_length=32, device=\"cuda:0\"):\n    model.eval()\n    batch_size = images.size(0)\n    results = []\n\n    base_model = model.get_base_model()\n\n    for i in range(batch_size):\n        single_image = images[i:i+1]\n\n        single_image = single_image.to(device=device, dtype=next(model.vision.parameters()).dtype)\n        if torch.isnan(single_image).any():\n            single_image = torch.nan_to_num(single_image, nan=0.0)\n\n        patch_embeds = model.vision(single_image)\n        patch_embeds = patch_embeds.to(model.qwen.dtype).to(device)\n        if torch.isnan(patch_embeds).any():\n            patch_embeds = torch.nan_to_num(patch_embeds, nan=0.0)\n\n        q_tok = model.tokenizer(\n            [questions[i]],\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=64\n        ).to(device)\n\n        text_embeds = base_model.get_input_embeddings()(q_tok.input_ids).to(model.qwen.dtype)\n        inputs_embeds = torch.cat([patch_embeds, text_embeds], dim=1)\n        patch_mask = torch.ones((1, patch_embeds.size(1)), device=device)\n        attention_mask = torch.cat([patch_mask, q_tok.attention_mask], dim=1)\n\n        out_ids = model.qwen.generate(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            max_new_tokens=7,\n            min_new_tokens=1,\n            do_sample=False,\n            num_beams=1,\n            repetition_penalty=1.5,\n            no_repeat_ngram_size=2,\n            eos_token_id=model.tokenizer.eos_token_id,\n            pad_token_id=model.tokenizer.pad_token_id\n        )\n\n        text = model.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n        if text.startswith(questions[i]):\n            text = text[len(questions[i]):].strip()\n        text = text.split('.')[0].split('!')[0].split('?')[0].strip()\n        if not text:\n            text = \"unknown\"\n\n        results.append(text)\n\n        del patch_embeds, text_embeds, inputs_embeds, attention_mask, out_ids, q_tok\n        torch.cuda.empty_cache()\n\n    return results\n\n@torch.no_grad()\ndef evaluate_bert_f1_and_examples(model, dataloader, device=\"cuda:0\", num_examples=3):\n    model = model.to(device)  # Ensure model is on the correct device\n    model.eval()\n    preds, refs, questions = [], [], []\n\n    for batch_idx, batch in enumerate(dataloader):\n        images = batch[\"image\"].to(device)\n        batch_questions = batch[\"question\"]\n        batch_answers = batch[\"answer\"]\n\n        batch_preds = generate_answer(model, images, batch_questions, device=device)\n        preds.extend(batch_preds)\n        refs.extend(batch_answers)\n        questions.extend(batch_questions)\n\n        if len(preds) >= num_examples:\n            break\n\n    print(\"\\nSample predictions:\")\n    for i in range(min(num_examples, len(preds))):\n        print(f\"\\n  Example {i + 1}:\")\n        print(f\"    Question:   {questions[i]}\")\n        print(f\"    Predicted:  {preds[i]}\")\n        print(f\"    Reference:  {refs[i]}\")\n\n    print(f\"\\nSkipping BERTScore F1 (library unavailable)\")\n    return 0.0\n\nif __name__ == \"__main__\":\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"Loading models...\")\n    tokenizer, qwen = load_qwen_frozen()\n    vision_model, image_processor = get_model(\n        encoder_choice=\"pubmedclip\",\n        mlp_output_dim=qwen.config.hidden_size,\n        hidden_dim=2048\n    )\n    model = MultiModalVQA(\n        vision_mlp_model=vision_model,\n        qwen=qwen,\n        tokenizer=tokenizer\n    )\n\n    train_loader = get_dataloader(\"vqa_rad\", image_processor, batch_size=16)\n    test_loader = get_dataloader(\"vqa_rad\", image_processor, batch_size=1, max_samples=20)\n\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_top_k=1,\n        filename=\"best-{epoch}-{val_loss:.4f}\",\n        save_weights_only=False\n    )\n\n    # ----------------- MLP Stage -----------------\n    print(\"\\nStarting MLP training...\")\n    lit_model = LitMultiModalVQA(\n        model=model,\n        lr=3e-5,\n        stage=\"mlp\"\n    )\n\n    trainer = pl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        precision=\"16-mixed\",\n        max_epochs=3,\n        callbacks=[checkpoint_callback, BertF1Callback(test_loader), CheckpointEveryEpoch(\"mlp\")],\n        log_every_n_steps=10,\n    )\n\n    trainer.fit(\n        lit_model,\n        train_dataloaders=train_loader,\n        val_dataloaders=test_loader,\n    )\n    model_mlp = lit_model.model\n    model_mlp.to(\"cuda:0\")\n    evaluate_bert_f1_and_examples(model_mlp, test_loader, device=\"cuda:0\", num_examples=3)\n\n    # ----------------- LoRA Stage -----------------\n    print(\"\\nApplying LoRA to Qwen...\")\n    model.qwen = apply_lora_to_qwen(model.qwen)\n\n    lit_model_lora = LitMultiModalVQA(\n        model=model,\n        lr=1e-4,\n        stage=\"lora\"\n    )\n\n    trainer = pl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        precision=\"16-mixed\",\n        max_epochs=2,\n        callbacks=[checkpoint_callback, BertF1Callback(test_loader), CheckpointEveryEpoch(\"lora\")],\n        log_every_n_steps=10,\n    )\n\n    trainer.fit(\n        lit_model_lora,\n        train_dataloaders=train_loader,\n        val_dataloaders=test_loader,\n    )\n\n    model_lora = lit_model_lora.model\n    model_lora.to(\"cuda:0\")\n    evaluate_bert_f1_and_examples(model_lora, test_loader, device=\"cuda:0\", num_examples=3)\n\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"Loading checkpoint and making predictions...\")\n    print(\"=\"*80)\n    \n    del model_lora, lit_model_lora, trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    tokenizer_new, qwen_new = load_qwen_frozen()\n    vision_model_new, image_processor_new = get_model(\n        encoder_choice=\"pubmedclip\",\n        mlp_output_dim=qwen_new.config.hidden_size,\n        hidden_dim=2048\n    )\n    model_new = MultiModalVQA(vision_model_new, qwen_new, tokenizer_new)\n    model_new.qwen = apply_lora_to_qwen(model_new.qwen)\n    \n    checkpoint_path = \"checkpoints/lora_epoch_1.pt\"\n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=\"cuda:0\")\n        model_new.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n    \n    model_new.to(\"cuda:0\").eval()\n    test_loader_inference = get_dataloader(\"vqa_rad\", image_processor_new, batch_size=1, max_samples=10)\n    \n    all_preds, all_refs, all_questions = [], [], []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(test_loader_inference):\n            images = batch[\"image\"].to(\"cuda:0\")\n            questions = batch[\"question\"]\n            answers = batch[\"answer\"]\n            \n            predictions = generate_answer(model_new, images, questions, device=\"cuda:0\")\n            all_preds.extend(predictions)\n            all_refs.extend(answers)\n            all_questions.extend(questions)\n            \n            print(f\"\\nExample {batch_idx + 1}:\")\n            print(f\"  Question:  {questions[0]}\")\n            print(f\"  Predicted: {predictions[0]}\")\n            print(f\"  Reference: {answers[0]}\")\n            print(f\"  Match:     {'✓' if predictions[0].lower().strip() == answers[0].lower().strip() else '✗'}\")\n    \n    exact_matches = sum(1 for p, r in zip(all_preds, all_refs) if p.lower().strip() == r.lower().strip())\n    print(f\"\\nExact match: {exact_matches}/{len(all_preds)} ({exact_matches/len(all_preds)*100:.2f}%)\")\n    \n    with open(\"predictions_output.json\", 'w') as f:\n        json.dump([{\"question\": q, \"predicted\": p, \"reference\": r} \n                   for q, p, r in zip(all_questions, all_preds, all_refs)], f, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:40:50.423438Z","iopub.execute_input":"2025-12-21T16:40:50.423703Z","iopub.status.idle":"2025-12-21T16:48:46.369363Z","shell.execute_reply.started":"2025-12-21T16:40:50.423675Z","shell.execute_reply":"2025-12-21T16:48:46.368728Z"}},"outputs":[{"name":"stderr","text":"2025-12-21 16:41:03.683487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766335263.872443      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766335263.927572      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766335264.370470      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766335264.370502      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766335264.370505      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766335264.370507      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Loading models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c88c40e23c004976b9679b7df7f0f065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1def73df16a4c60a258c7a67655e954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7bda918436940608506127382204479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a2ee250dfa4f90ba9c121a453d3043"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de0b242e5264d739fce674ff0ab8238"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d37d8b148ce476988bcbc49b0fcbcf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b640f64bb647a3a922c664bf23037e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a5a2abf88764c10ab400cd898d22904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5451c2d6b704ac58598b5ebe9e28115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37ef6aaead3e4a33a28692a65096b8f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e677e1632644c5ac6eec5e130cad60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558425116ba6408f9e1e300f150debc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-eb8844602202be(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0ce36b3937418db87de4c922fa45d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001-e5bc3d208bb4dee(…):   0%|          | 0.00/10.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f2165c6b994e4f9142c02db04cf007"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1793 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159163ab6e544edb91d10cd8c2862a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/451 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fce58852e6d495f88b3bcabdea09519"}},"metadata":{}},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","output_type":"stream"},{"name":"stdout","text":"\nStarting MLP training...\n","output_type":"stream"},{"name":"stderr","text":"INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ MultiModalVQA │  555 M │ train │     0 │\n└───┴───────┴───────────────┴────────┴───────┴───────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ MultiModalVQA │  555 M │ train │     0 │\n└───┴───────┴───────────────┴────────┴───────┴───────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 3.7 M                                                                                            \n\u001b[1mNon-trainable params\u001b[0m: 551 M                                                                                        \n\u001b[1mTotal params\u001b[0m: 555 M                                                                                                \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 2.2 K                                                                      \n\u001b[1mModules in train mode\u001b[0m: 9                                                                                           \n\u001b[1mModules in eval mode\u001b[0m: 472                                                                                          \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 3.7 M                                                                                            \n<span style=\"font-weight: bold\">Non-trainable params</span>: 551 M                                                                                        \n<span style=\"font-weight: bold\">Total params</span>: 555 M                                                                                                \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 2.2 K                                                                      \n<span style=\"font-weight: bold\">Modules in train mode</span>: 9                                                                                           \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 472                                                                                          \n<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58d15036da64109bb02bffd69803fa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Checkpoint saved: checkpoints/mlp_epoch_1.pt\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checkpoint saved: checkpoints/mlp_epoch_1.pt\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Checkpoint saved: checkpoints/mlp_epoch_2.pt\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checkpoint saved: checkpoints/mlp_epoch_2.pt\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Checkpoint saved: checkpoints/mlp_epoch_3.pt\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checkpoint saved: checkpoints/mlp_epoch_3.pt\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n","output_type":"stream"},{"name":"stdout","text":"\nSample predictions:\n\n  Example 1:\n    Question:   how was this image taken?\n    Predicted:  mri-ct, 1\n    Reference:  mri\n\n  Example 2:\n    Question:   are the lungs affected?\n    Predicted:  no, lung affected yes\nl\n    Reference:  no\n\n  Example 3:\n    Question:   are the lungs normal appearing?\n    Predicted:  no, yes\n    Reference:  no\n\nSkipping BERTScore F1 (library unavailable)\n\nApplying LoRA to Qwen...\ntrainable params: 3,784,704 || all params: 467,772,416 || trainable%: 0.8091\n","output_type":"stream"},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━┓\n┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━┩\n│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ MultiModalVQA │  558 M │ eval │     0 │\n└───┴───────┴───────────────┴────────┴──────┴───────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ MultiModalVQA │  558 M │ eval │     0 │\n└───┴───────┴───────────────┴────────┴──────┴───────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 7.5 M                                                                                            \n\u001b[1mNon-trainable params\u001b[0m: 551 M                                                                                        \n\u001b[1mTotal params\u001b[0m: 558 M                                                                                                \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 2.2 K                                                                      \n\u001b[1mModules in train mode\u001b[0m: 1682                                                                                        \n\u001b[1mModules in eval mode\u001b[0m: 481                                                                                          \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 7.5 M                                                                                            \n<span style=\"font-weight: bold\">Non-trainable params</span>: 551 M                                                                                        \n<span style=\"font-weight: bold\">Total params</span>: 558 M                                                                                                \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 2.2 K                                                                      \n<span style=\"font-weight: bold\">Modules in train mode</span>: 1682                                                                                        \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 481                                                                                          \n<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aeb1aa6b3fb4dbea400bfda09aba5f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Checkpoint saved: checkpoints/lora_epoch_1.pt\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checkpoint saved: checkpoints/lora_epoch_1.pt\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Checkpoint saved: checkpoints/lora_epoch_2.pt\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checkpoint saved: checkpoints/lora_epoch_2.pt\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"\nSample predictions:\n\n  Example 1:\n    Question:   how was this image taken?\n    Predicted:  air with air of the and,\n    Reference:  mri\n\n  Example 2:\n    Question:   where is the mass?\n    Predicted:  bilateral lobe the left side\n    Reference:  pineal region\n\n  Example 3:\n    Question:   which organ system is abnormal in this image?\n    Predicted:  card liver and the ofth kidney\n    Reference:  cardiovascular\n\nSkipping BERTScore F1 (library unavailable)\n\n================================================================================\nLoading checkpoint and making predictions...\n================================================================================\ntrainable params: 3,784,704 || all params: 467,772,416 || trainable%: 0.8091\nLoaded checkpoint from epoch 1\n\nExample 1:\n  Question:  is this image in the transverse plane?\n  Predicted: no,yes\n  Reference: yes\n  Match:     ✗\n\nExample 2:\n  Question:  how was this image taken?\n  Predicted: axialicaliacalip\n  Reference: mri\n  Match:     ✗\n\nExample 3:\n  Question:  what is the location of the mass?\n  Predicted: right side of the brain and right\n  Reference: pineal region\n  Match:     ✗\n\nExample 4:\n  Question:  is the lesion causing significant brainstem herniation?\n  Predicted: no, yes\n  Reference: no\n  Match:     ✗\n\nExample 5:\n  Question:  what abnormality is seen?\n  Predicted: calcularisus,ct and\n  Reference: blind-ending loop of bowel arising from the cecum\n  Match:     ✗\n\nExample 6:\n  Question:  are regions of the brain infarcted?\n  Predicted: yes, the brain hemisphere and right\n  Reference: yes\n  Match:     ✗\n\nExample 7:\n  Question:  where is the mass?\n  Predicted: right side brain,paralum\n  Reference: pineal region\n  Match:     ✗\n\nExample 8:\n  Question:  which organ system is abnormal in this image?\n  Predicted: right kidneyth lung and right kidneys\n  Reference: cardiovascular\n  Match:     ✗\n\nExample 9:\n  Question:  what is the condition of the patient\n  Predicted: with the right kidney and left renal\n  Reference: blind loop syndrome\n  Match:     ✗\n\nExample 10:\n  Question:  are the lungs normal appearing?\n  Predicted: no yes no\nnormal appearing,\n  Reference: no\n  Match:     ✗\n\nExact match: 0/10 (0.00%)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q aiogram","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:48:46.375481Z","iopub.execute_input":"2025-12-21T16:48:46.375659Z","iopub.status.idle":"2025-12-21T16:48:51.023096Z","shell.execute_reply.started":"2025-12-21T16:48:46.375641Z","shell.execute_reply":"2025-12-21T16:48:51.022294Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.4/698.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.49.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import asyncio\nfrom aiogram import Bot, Dispatcher, types\nfrom aiogram.filters import CommandStart\nfrom aiogram.types import FSInputFile\nimport nest_asyncio\n\n\ndef load_model():\n    gc.collect()\n    torch.cuda.empty_cache()\n    tokenizer, qwen = load_qwen_frozen()\n    vision_model, image_processor = get_model(\n        encoder_choice=\"pubmedclip\",\n        mlp_output_dim=qwen.config.hidden_size,\n        hidden_dim=2048\n    )\n    model = MultiModalVQA(\n        vision_mlp_model=vision_model,\n        qwen=qwen,\n        tokenizer=tokenizer\n    )\n    model.qwen = apply_lora_to_qwen(model.qwen)\n    \n    checkpoint_path = \"checkpoints/lora_epoch_1.pt\"\n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=\"cuda:0\")\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n    else:\n        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}. Please train the model first.\")\n    \n    model.to(\"cuda:0\").eval()\n    return model, image_processor\n\n# Telegram Bot Code with aiogram\nTOKEN = '6539903139:AAF3IRqIXQuoIOQZs7iMis83PuYp9q7eTQA'  \n\nmodel, image_processor = load_model()\n\nbot = Bot(token=TOKEN)\ndp = Dispatcher()\n\n@dp.message(CommandStart())\nasync def start(message: types.Message):\n    await message.reply('Send me an image with a caption as the question, and I\\'ll answer using the VQA model!')\n\n@dp.message()\nasync def handle_photo(message: types.Message):\n    print(\"Received message from user:\", message.from_user.id)\n    print(\"Message has photo:\", bool(message.photo))\n    print(\"Message has caption:\", bool(message.caption))\n    if message.photo and message.caption:\n        try:\n            print(\"Starting photo processing...\")\n            # Download the photo\n            photo = message.photo[-1]  # Get the highest resolution\n            print(\"Selected photo size:\", photo.width, \"x\", photo.height)\n            file = await bot.get_file(photo.file_id)\n            print(\"Got file info:\", file.file_id, file.file_path)\n            photo_path = 'temp_image.jpg'\n            await bot.download_file(file.file_path, photo_path)\n            print(\"Downloaded photo to:\", photo_path)\n            \n            # Process the image\n            print(\"Opening image...\")\n            image = Image.open(photo_path).convert(\"RGB\")\n            print(\"Resizing image...\")\n            image = image.resize((224, 224), Image.LANCZOS)\n            print(\"Processing image with image_processor...\")\n            processed_image = image_processor(image, return_tensors=\"pt\").pixel_values.to(\"cuda:0\")\n            print(\"Processed image shape:\", processed_image.shape)\n            \n            # Get the question (caption)\n            question = message.caption\n            print(\"Question:\", question)\n            \n            # Generate answer\n            print(\"Generating answer...\")\n            prediction = generate_answer(model, processed_image, [question])[0]\n            print(\"Generated prediction:\", prediction)\n            \n            # Reply\n            print(\"Sending reply...\")\n            await message.reply(f\"Question: {question}\\nAnswer: {prediction}\")\n            print(\"Reply sent.\")\n            \n            # Clean up\n            print(\"Cleaning up...\")\n            os.remove(photo_path)\n            del processed_image\n            torch.cuda.empty_cache()\n            print(\"Cleanup complete.\")\n        except Exception as e:\n            print(\"Error during processing:\", str(e))\n            await message.reply(f\"Error processing photo: {str(e)}\")\n    else:\n        print(\"Message does not have photo or caption. Sending reminder.\")\n        await message.reply('Please send an image with a caption as the question.')\n\nasync def main():\n    await dp.start_polling(bot)\n\nif __name__ == '__main__':\n    nest_asyncio.apply()\n    asyncio.run(main())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:48:56.654081Z","iopub.execute_input":"2025-12-21T16:48:56.654389Z"}},"outputs":[{"name":"stdout","text":"trainable params: 3,784,704 || all params: 467,772,416 || trainable%: 0.8091\nLoaded checkpoint from epoch 1\nReceived message from user: 5410612788\nMessage has photo: True\nMessage has caption: True\nStarting photo processing...\nSelected photo size: 514 x 626\nGot file info: AgACAgIAAxkBAAOBaUgb0TyG5lt7ywn5p3w0Gz2fEZcAAqgMaxsMSkBKwcLn6qu5bksBAAMCAAN4AAM2BA photos/file_0.jpg\nDownloaded photo to: temp_image.jpg\nOpening image...\nResizing image...\nProcessing image with image_processor...\nProcessed image shape: torch.Size([1, 3, 224, 224])\nQuestion: hello how are you\nGenerating answer...\nGenerated prediction: axial andyes,no\nSending reply...\nReply sent.\nCleaning up...\nCleanup complete.\nReceived message from user: 5410612788\nMessage has photo: True\nMessage has caption: True\nStarting photo processing...\nSelected photo size: 800 x 757\nGot file info: AgACAgIAAxkBAAOOaUglOqHpDGe6ZEaHfrfteO16cX4AAhkNaxsMSkBKfQPsH6joXBgBAAMCAAN4AAM2BA photos/file_1.jpg\nDownloaded photo to: temp_image.jpg\nOpening image...\nResizing image...\nProcessing image with image_processor...\nProcessed image shape: torch.Size([1, 3, 224, 224])\nQuestion: do i have lung cancer?\nGenerating answer...\nGenerated prediction: no yes no\nSending reply...\nReply sent.\nCleaning up...\nCleanup complete.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}