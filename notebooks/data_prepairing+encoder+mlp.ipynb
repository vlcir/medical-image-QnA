{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95cec12ecfe74ccc90a656f4abf6fb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4a0afee2d024a49b34c60a4b3f66e6c",
              "IPY_MODEL_e75feb5ce21241cd9fcd02f7d5b7e624",
              "IPY_MODEL_7d7dc8827a17480e8a5ed6294efa59a2"
            ],
            "layout": "IPY_MODEL_36b849ed91a04e8b868757b8d30ee580"
          }
        },
        "a4a0afee2d024a49b34c60a4b3f66e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a41eb36a2ee44f6086e847e907b5799c",
            "placeholder": "​",
            "style": "IPY_MODEL_41820aafcb074d6996c8d37a08259ee8",
            "value": "Fetching 8 files: 100%"
          }
        },
        "e75feb5ce21241cd9fcd02f7d5b7e624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22f8d1ae6e9e4628aed3c50364555c4c",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_239afba0262c4352be83aa2a20edb568",
            "value": 8
          }
        },
        "7d7dc8827a17480e8a5ed6294efa59a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e6f7b2e76f48dca0efb3e2ba30e457",
            "placeholder": "​",
            "style": "IPY_MODEL_5f5268ee3ceb465b8029f5ff0a121c66",
            "value": " 8/8 [00:00&lt;00:00, 569.32it/s]"
          }
        },
        "36b849ed91a04e8b868757b8d30ee580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a41eb36a2ee44f6086e847e907b5799c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41820aafcb074d6996c8d37a08259ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22f8d1ae6e9e4628aed3c50364555c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "239afba0262c4352be83aa2a20edb568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39e6f7b2e76f48dca0efb3e2ba30e457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f5268ee3ceb465b8029f5ff0a121c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c98e8756a2449ef9eb55696afe89227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efb3b6e0dc0b420db274a99d6e9a4fe6",
              "IPY_MODEL_da7b1486b4bf4b94ac87c326dafba268",
              "IPY_MODEL_abfb6a4b3e2a425c901e71aed6823c35"
            ],
            "layout": "IPY_MODEL_9b29086b9fee487b902839f24dfcc6ca"
          }
        },
        "efb3b6e0dc0b420db274a99d6e9a4fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b21b97f59d144f78701e69ca069fd0e",
            "placeholder": "​",
            "style": "IPY_MODEL_7e4f16beb91c4482ab7c8a4d72e7cab6",
            "value": "Fetching 8 files: 100%"
          }
        },
        "da7b1486b4bf4b94ac87c326dafba268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eda1d297b3a47a9898555aa77469003",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_007fb49da02643a79afa59da4a37408d",
            "value": 8
          }
        },
        "abfb6a4b3e2a425c901e71aed6823c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aafbec6573a4e71832a85b36098099f",
            "placeholder": "​",
            "style": "IPY_MODEL_6d3951f9a1524d539bfa1a0b1f73d20d",
            "value": " 8/8 [00:00&lt;00:00, 757.06it/s]"
          }
        },
        "9b29086b9fee487b902839f24dfcc6ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b21b97f59d144f78701e69ca069fd0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4f16beb91c4482ab7c8a4d72e7cab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eda1d297b3a47a9898555aa77469003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "007fb49da02643a79afa59da4a37408d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0aafbec6573a4e71832a85b36098099f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d3951f9a1524d539bfa1a0b1f73d20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "скип этой хуйни, чекай ласт ячейку(это на всякий случай оставил вдруг поможет)"
      ],
      "metadata": {
        "id": "QFF478AiTflT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK1EBX7ATW6E",
        "outputId": "1450be24-f2cb-4803-ab85-9670fb048543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install transformers datasets accelerate\n",
        "!pip install Pillow\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Шаг 1: Установка зависимостей и проверка GPU (ФИНАЛЬНАЯ ВЕРСИЯ)\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q transformers datasets accelerate Pillow huggingface_hub\n",
        "\n",
        "import torch\n",
        "print(f\"Используемое устройство: cuda\" if torch.cuda.is_available() else \"Используемое устройство: cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoQhSZNhfeGn",
        "outputId": "0c6ff8e4-fc9d-4725-e510-0348d9ae8c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используемое устройство: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloaders.py\n",
        "# @title Шаг 3: Определяем классы Dataloader'ов (Исправленная версия)\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "class SlakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Даталоадер для датасета SLAKE с Hugging Face.\n",
        "    Фильтрует данные, оставляя только англоязычные вопросы.\n",
        "    \"\"\"\n",
        "    def __init__(self, processor, split='train'):\n",
        "        print(f\"Loading SLAKE dataset for split: {split}\")\n",
        "        self.processor = processor\n",
        "        full_dataset = load_dataset(\"BoKelvin/SLAKE\", split=split)\n",
        "\n",
        "        # --- ИСПРАВЛЕНИЕ ЗДЕСЬ ---\n",
        "        # Фильтруем по столбцу 'q_lang', как вы и указали.\n",
        "        self.dataset = full_dataset.filter(lambda example: example['q_lang'] == 'en')\n",
        "\n",
        "        print(f\"Original size: {len(full_dataset)}, Filtered size (en only): {len(self.dataset)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image'].convert(\"RGB\")\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        processed_image = self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        return {\"image\": processed_image, \"question\": question, \"answer\": answer}\n",
        "\n",
        "\n",
        "class VQARADDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Даталоадер для датасета VQA-RAD (скачанного вручную).\n",
        "    \"\"\"\n",
        "    def __init__(self, processor, root_dir, split='train'):\n",
        "        print(f\"Loading VQA-RAD dataset for split: {split}\")\n",
        "        self.processor = processor\n",
        "        self.root_dir = root_dir\n",
        "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
        "        json_path = os.path.join(self.root_dir, f'{split}.json')\n",
        "        with open(json_path, 'r') as f:\n",
        "            self.dataset = json.load(f)\n",
        "        print(f\"Dataset size: {len(self.dataset)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image_path = os.path.join(self.image_dir, item['image_name'])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        processed_image = self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        return {\"image\": processed_image, \"question\": question, \"answer\": answer}\n",
        "\n"
      ],
      "metadata": {
        "id": "MDMybmGFZB9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Шаг 4: Определяем архитектуру модели (Vision + MLP) (ФИНАЛЬНАЯ ВЕРСИЯ)\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, CLIPVisionModel\n",
        "\n",
        "class VisionMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Финальная версия модели, которая корректно загружает ОБА типа энкодеров,\n",
        "    используя официальные методы.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_path, mlp_embedding_dim=4096):\n",
        "        super().__init__()\n",
        "        self.encoder_path = encoder_path\n",
        "\n",
        "        # --- КЛЮЧЕВОЕ ИЗМЕНЕНИЕ: УМНЫЙ ЗАГРУЗЧИК ---\n",
        "        if \"microsoft/BiomedCLIP\" in encoder_path:\n",
        "            self.model_type = 'biomed_clip'\n",
        "            print(\"Detected BiomedCLIP (custom model). Loading with `trust_remote_code=True`.\")\n",
        "            # Это ОФИЦИАЛЬНЫЙ способ загрузки этой модели.\n",
        "            # Он выполняет код из репозитория модели для ее правильной сборки.\n",
        "            self.vision_model = AutoModel.from_pretrained(encoder_path, trust_remote_code=True)\n",
        "            # У этой модели скрытое состояние 768\n",
        "            vision_hidden_size = 768\n",
        "            print(\"Custom BiomedCLIP model loaded successfully.\")\n",
        "        else:\n",
        "            self.model_type = 'standard_clip'\n",
        "            print(f\"Detected standard Transformers model: {encoder_path}\")\n",
        "            # Для стандартных моделей, как pubmed-clip, загружаем только vision-часть\n",
        "            self.vision_model = CLIPVisionModel.from_pretrained(encoder_path)\n",
        "            vision_hidden_size = self.vision_model.config.hidden_size\n",
        "            print(\"Standard CLIP vision model loaded successfully.\")\n",
        "\n",
        "        # MLP-проектор остается без изменений\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(vision_hidden_size, vision_hidden_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(vision_hidden_size * 4, mlp_embedding_dim)\n",
        "        )\n",
        "        print(f\"MLP configured: {vision_hidden_size} -> {mlp_embedding_dim}\")\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # --- УМНЫЙ FORWARD PASS ---\n",
        "        if self.model_type == 'biomed_clip':\n",
        "            # У этой кастомной модели есть свой метод для получения эмбеддингов изображений\n",
        "            image_features = self.vision_model.get_image_features(pixel_values=pixel_values)\n",
        "        else: # standard_clip\n",
        "            # Стандартная модель возвращает объект вывода, из которого мы берем pooler_output\n",
        "            outputs = self.vision_model(pixel_values=pixel_values)\n",
        "            image_features = outputs.pooler_output\n",
        "\n",
        "        return self.mlp(image_features)\n",
        "\n",
        "print(\"Класс VisionMLP (ФИНАЛЬНАЯ ВЕРСИЯ) определен. Теперь можно запускать Шаг 5.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdSNPZH-ZDxt",
        "outputId": "e2e0f60f-9d91-4b27-a937-b9807e4d7fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Класс VisionMLP (ФИНАЛЬНАЯ ВЕРСИЯ) определен. Теперь можно запускать Шаг 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Шаг 5: Запуск основного пайплайна (ФИНАЛЬНАЯ ВЕРСИЯ)\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# --- Переопределяем даталоадеры здесь для простоты ---\n",
        "class SlakeDataset(Dataset):\n",
        "    def __init__(self, processor, split='train'):\n",
        "        self.processor = processor\n",
        "        full_dataset = load_dataset(\"BoKelvin/SLAKE\", split=split)\n",
        "        self.dataset = full_dataset.filter(lambda example: example['q_lang'] == 'en')\n",
        "    def __len__(self): return len(self.dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image'].convert(\"RGB\")\n",
        "        # Процессор возвращает dict, нам нужны 'pixel_values'\n",
        "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
        "        return {\"image\": processed['pixel_values'].squeeze(0), \"question\": item['question'], \"answer\": item['answer']}\n",
        "\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, processor, root_dir, split='train'):\n",
        "        self.processor = processor\n",
        "        self.root_dir = root_dir\n",
        "        self.image_dir = os.path.join(root_dir, 'images')\n",
        "        with open(os.path.join(root_dir, f'{split}.json'), 'r') as f: self.dataset = json.load(f)\n",
        "    def __len__(self): return len(self.dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = Image.open(os.path.join(self.image_dir, item['image_name'])).convert(\"RGB\")\n",
        "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
        "        return {\"image\": processed['pixel_values'].squeeze(0), \"question\": item['question'], \"answer\": item['answer']}\n",
        "\n",
        "# --- Конфигурация запуска ---\n",
        "ENCODER_CHOICE = 'biomedclip' # 'biomedclip' или 'pubmedclip'\n",
        "DATASET_CHOICE = 'slake'      # 'slake' или 'vqa_rad'\n",
        "\n",
        "def main():\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    MODEL_PATHS = {\n",
        "        'biomedclip': 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224',\n",
        "        'pubmedclip': 'flaviagiammarino/pubmed-clip-vit-base-patch32'\n",
        "    }\n",
        "    ENCODER_PATH = MODEL_PATHS[ENCODER_CHOICE]\n",
        "\n",
        "    print(f\"--- Running pipeline with {ENCODER_CHOICE} on {DATASET_CHOICE} dataset ---\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # 1. Загрузка препроцессора. AutoProcessor универсален.\n",
        "    # `trust_remote_code=True` нужен для BiomedCLIP, но не повредит и pubmedclip.\n",
        "    processor = AutoProcessor.from_pretrained(ENCODER_PATH, trust_remote_code=True)\n",
        "\n",
        "    # 2. Инициализация датасета\n",
        "    if DATASET_CHOICE == 'slake': dataset = SlakeDataset(processor=processor, split='train')\n",
        "    else: dataset = VQARADDataset(processor=processor, root_dir=\"data/vqa_rad/VQA_RAD_Dataset\", split='train')\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "    # 3. Инициализация модели. Теперь она сама все знает.\n",
        "    model = VisionMLP(encoder_path=ENCODER_PATH, mlp_embedding_dim=4096).to(DEVICE)\n",
        "\n",
        "    # Заморозка весов. `model.vision_model` теперь наш стандартизированный атрибут\n",
        "    for param in model.vision_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 4. Прогон одного батча\n",
        "    print(\"\\n--- Processing one batch ---\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(dataloader))\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        image_embeddings = model(images)\n",
        "\n",
        "        print(f\"\\nBatch loaded. Shape: {images.shape}\")\n",
        "        print(f\"Output embeddings shape: {image_embeddings.shape}\")\n",
        "        assert image_embeddings.shape == (images.shape[0], 4096)\n",
        "        print(\"\\nShape check PASSED.\")\n",
        "\n",
        "    print(\"\\n--- Pipeline execution successful! ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "IDgum5ryZIPL",
        "outputId": "9a8feb41-59e5-4b61-f267-dfece67bef97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running pipeline with biomedclip on slake dataset ---\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected BiomedCLIP (custom model). Loading with `trust_remote_code=True`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1132642210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1132642210.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# 3. Инициализация модели. Теперь она сама все знает.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisionMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mENCODER_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_embedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Заморозка весов. `model.vision_model` теперь наш стандартизированный атрибут\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3286519014.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoder_path, mlp_embedding_dim)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Это ОФИЦИАЛЬНЫЙ способ загрузки этой модели.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Он выполняет код из репозитория модели для ее правильной сборки.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# У этой модели скрытое состояние 768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mvision_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4898\u001b[0m             )\n\u001b[1;32m   4899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4900\u001b[0;31m         checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n\u001b[0m\u001b[1;32m   4901\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4902\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                             )\n\u001b[1;32m   1147\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                             raise OSError(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                                 \u001b[0;34mf\"{pretrained_model_name_or_path} does not appear to have a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                                 \u001b[0;34mf\" {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "прогоняем биоклип на слейке"
      ],
      "metadata": {
        "id": "UypGZgxPpXQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# КОД.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. УСТАНОВКА ЗАВИСИМОСТЕЙ ---\n",
        "print(\"--- 1. Установка правильных зависимостей (включая open_clip_torch) ---\")\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers datasets accelerate Pillow huggingface_hub\n",
        "!pip install -q open_clip_torch\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPVisionModel, AutoImageProcessor\n",
        "from huggingface_hub import snapshot_download, hf_hub_download\n",
        "import open_clip\n",
        "\n",
        "# --- 2. DATALOADER ДЛЯ SLAKE ---\n",
        "class SlakeDataset(Dataset):\n",
        "    def __init__(self, image_processor, hf_repo_id='BoKelvin/SLAKE', split='train'):\n",
        "        self.image_processor = image_processor\n",
        "        self.root_dir = snapshot_download(repo_id=hf_repo_id, repo_type='dataset')\n",
        "        img_dir_path = os.path.join(self.root_dir, 'imgs')\n",
        "        if not os.path.exists(img_dir_path):\n",
        "            os.system(f\"unzip -q -o {os.path.join(self.root_dir, 'imgs.zip')} -d {self.root_dir}\")\n",
        "        json_path = os.path.join(self.root_dir, f\"{split}.json\")\n",
        "        with open(json_path, 'r') as f:\n",
        "            full_data = json.load(f)\n",
        "        self.dataset = [item for item in full_data if item['q_lang'] == 'en']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image_name = item['img_name']\n",
        "        image_path = os.path.join(self.root_dir, 'imgs', image_name)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        processed_image = self.image_processor(image)\n",
        "        if processed_image.dim() > 3:\n",
        "            processed_image = processed_image.squeeze(0)\n",
        "        return {\"image\": processed_image, \"question\": item['question'], \"answer\": item['answer']}\n",
        "\n",
        "# --- 3. КЛАСС МОДЕЛИ ---\n",
        "class VisionMLP(nn.Module):\n",
        "    def __init__(self, vision_encoder, encoder_output_dim, mlp_output_dim):\n",
        "        super().__init__()\n",
        "        self.vision_encoder = vision_encoder\n",
        "        self.model_type = 'open_clip' if isinstance(vision_encoder, open_clip.model.VisionTransformer) else 'transformers'\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(encoder_output_dim, encoder_output_dim * 4), nn.GELU(),\n",
        "            nn.Linear(encoder_output_dim * 4, mlp_output_dim))\n",
        "        print(f\"--- MLP сконфигурирован: {encoder_output_dim} -> {mlp_output_dim} ---\")\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        if self.model_type == 'open_clip':\n",
        "            image_features = self.vision_encoder(pixel_values)\n",
        "        else: # transformers\n",
        "            image_features = self.vision_encoder(pixel_values=pixel_values).pooler_output\n",
        "        return self.mlp(image_features)\n",
        "\n",
        "# --- 4. ОСНОВНАЯ ФУНКЦИЯ ЗАПУСКА ---\n",
        "def main():\n",
        "    ENCODER_CHOICE = 'biomedclip' # 'biomedclip' или 'pubmedclip'\n",
        "    MLP_OUTPUT_DIM = 4096\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    MODEL_PATHS = {\n",
        "        'biomedclip': 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224',\n",
        "        'pubmedclip': 'flaviagiammarino/pubmed-clip-vit-base-patch32'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- 4. Запускаю пайплайн: {ENCODER_CHOICE} на slake ---\")\n",
        "\n",
        "    if ENCODER_CHOICE == 'biomedclip':\n",
        "        print(\"--- Загрузка BiomedCLIP... ---\")\n",
        "        model_obj, _, image_processor = open_clip.create_model_and_transforms('ViT-B-16', pretrained='openai')\n",
        "        checkpoint_path = hf_hub_download(repo_id=MODEL_PATHS['biomedclip'], filename=\"open_clip_pytorch_model.bin\")\n",
        "        model_obj.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE), strict=False)\n",
        "        vision_encoder = model_obj.visual\n",
        "\n",
        "        # --- ФИНАЛЬНОЕ ИСПРАВЛЕНИЕ ЗДЕСЬ ---\n",
        "        # Размерность выхода берется из матрицы проекции\n",
        "        encoder_output_dim = model_obj.visual.proj.shape[1]\n",
        "\n",
        "        print(f\"--- BiomedCLIP загружен. Фактическая размерность выхода: {encoder_output_dim} ---\")\n",
        "    else: # pubmedclip\n",
        "        print(\"--- Загрузка PubMedCLIP... ---\")\n",
        "        vision_encoder = CLIPVisionModel.from_pretrained(MODEL_PATHS['pubmedclip'])\n",
        "        image_processor = AutoImageProcessor.from_pretrained(MODEL_PATHS['pubmedclip'])\n",
        "        encoder_output_dim = vision_encoder.config.hidden_size\n",
        "        print(f\"--- PubMedCLIP загружен. Фактическая размерность выхода: {encoder_output_dim} ---\")\n",
        "\n",
        "    dataset = SlakeDataset(image_processor=image_processor)\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "    model = VisionMLP(\n",
        "        vision_encoder=vision_encoder,\n",
        "        encoder_output_dim=encoder_output_dim,\n",
        "        mlp_output_dim=MLP_OUTPUT_DIM\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    for param in model.vision_encoder.parameters(): param.requires_grad = False\n",
        "\n",
        "    print(\"\\n--- 5. Обрабатываю один батч... ---\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(dataloader))\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        image_embeddings = model(images)\n",
        "\n",
        "        print(f\"\\n- Форма входного батча: {images.shape}\")\n",
        "        print(f\"- Форма выходных эмбеддингов: {image_embeddings.shape}\")\n",
        "        assert image_embeddings.shape[1] == MLP_OUTPUT_DIM, \"Размерность эмбеддинга неверна!\"\n",
        "        print(\"\\n--- Проверка формы пройдена. ---\")\n",
        "\n",
        "    print(\"\\n\\n*** ПАЙПЛАЙН УСПЕШНО ВЫПОЛНЕН ***\")\n",
        "\n",
        "# --- 5. ЗАПУСК ---\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "95cec12ecfe74ccc90a656f4abf6fb68",
            "a4a0afee2d024a49b34c60a4b3f66e6c",
            "e75feb5ce21241cd9fcd02f7d5b7e624",
            "7d7dc8827a17480e8a5ed6294efa59a2",
            "36b849ed91a04e8b868757b8d30ee580",
            "a41eb36a2ee44f6086e847e907b5799c",
            "41820aafcb074d6996c8d37a08259ee8",
            "22f8d1ae6e9e4628aed3c50364555c4c",
            "239afba0262c4352be83aa2a20edb568",
            "39e6f7b2e76f48dca0efb3e2ba30e457",
            "5f5268ee3ceb465b8029f5ff0a121c66"
          ]
        },
        "id": "NnN2U5Fyb4Zj",
        "outputId": "7e160338-34e9-4e68-e109-8af4c7708abd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Установка правильных зависимостей (включая open_clip_torch) ---\n",
            "\n",
            "--- 4. Запускаю пайплайн: biomedclip на slake ---\n",
            "--- Загрузка BiomedCLIP... ---\n",
            "--- BiomedCLIP загружен. Фактическая размерность выхода: 512 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95cec12ecfe74ccc90a656f4abf6fb68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MLP сконфигурирован: 512 -> 4096 ---\n",
            "\n",
            "--- 5. Обрабатываю один батч... ---\n",
            "\n",
            "- Форма входного батча: torch.Size([4, 3, 224, 224])\n",
            "- Форма выходных эмбеддингов: torch.Size([4, 4096])\n",
            "\n",
            "--- Проверка формы пройдена. ---\n",
            "\n",
            "\n",
            "*** ПАЙПЛАЙН УСПЕШНО ВЫПОЛНЕН ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "прогоняем пубмед"
      ],
      "metadata": {
        "id": "1MT_iIk-pcit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# КОД.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. УСТАНОВКА ЗАВИСИМОСТЕЙ ---\n",
        "print(\"--- 1. Установка правильных зависимостей (включая open_clip_torch) ---\")\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers datasets accelerate Pillow huggingface_hub\n",
        "!pip install -q open_clip_torch\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPVisionModel, AutoImageProcessor\n",
        "from huggingface_hub import snapshot_download, hf_hub_download\n",
        "import open_clip\n",
        "\n",
        "# --- 2. DATALOADER ДЛЯ SLAKE (С ФИНАЛЬНЫМ ИСПРАВЛЕНИЕМ) ---\n",
        "class SlakeDataset(Dataset):\n",
        "    def __init__(self, image_processor, hf_repo_id='BoKelvin/SLAKE', split='train'):\n",
        "        self.image_processor = image_processor\n",
        "        self.root_dir = snapshot_download(repo_id=hf_repo_id, repo_type='dataset')\n",
        "        img_dir_path = os.path.join(self.root_dir, 'imgs')\n",
        "        if not os.path.exists(img_dir_path):\n",
        "            os.system(f\"unzip -q -o {os.path.join(self.root_dir, 'imgs.zip')} -d {self.root_dir}\")\n",
        "        json_path = os.path.join(self.root_dir, f\"{split}.json\")\n",
        "        with open(json_path, 'r') as f:\n",
        "            full_data = json.load(f)\n",
        "        self.dataset = [item for item in full_data if item['q_lang'] == 'en']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image_name = item['img_name']\n",
        "        image_path = os.path.join(self.root_dir, 'imgs', image_name)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Вызываем процессор. Для transformers он вернет объект, для open_clip - тензор\n",
        "        processed_output = self.image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "        # --- КЛЮЧЕВОЕ ИСПРАВЛЕНИЕ ЗДЕСЬ ---\n",
        "        # Унифицируем выход: всегда извлекаем тензор, если он внутри объекта\n",
        "        if hasattr(processed_output, 'pixel_values'):\n",
        "            processed_image = processed_output.pixel_values\n",
        "        else:\n",
        "            processed_image = processed_output\n",
        "\n",
        "        # Убираем лишнюю размерность батча, которую добавляет процессор\n",
        "        if processed_image.dim() > 3:\n",
        "            processed_image = processed_image.squeeze(0)\n",
        "\n",
        "        return {\"image\": processed_image, \"question\": item['question'], \"answer\": item['answer']}\n",
        "\n",
        "# --- 3. КЛАСС МОДЕЛИ ---\n",
        "class VisionMLP(nn.Module):\n",
        "    def __init__(self, vision_encoder, encoder_output_dim, mlp_output_dim):\n",
        "        super().__init__()\n",
        "        self.vision_encoder = vision_encoder\n",
        "        self.model_type = 'open_clip' if isinstance(vision_encoder, open_clip.model.VisionTransformer) else 'transformers'\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(encoder_output_dim, encoder_output_dim * 4), nn.GELU(),\n",
        "            nn.Linear(encoder_output_dim * 4, mlp_output_dim))\n",
        "        print(f\"--- MLP сконфигурирован: {encoder_output_dim} -> {mlp_output_dim} ---\")\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        if self.model_type == 'open_clip':\n",
        "            image_features = self.vision_encoder(pixel_values)\n",
        "        else: # transformers\n",
        "            image_features = self.vision_encoder(pixel_values=pixel_values).pooler_output\n",
        "        return self.mlp(image_features)\n",
        "\n",
        "# --- 4. ОСНОВНАЯ ФУНКЦИЯ ЗАПУСКА ---\n",
        "def main():\n",
        "    ENCODER_CHOICE = 'pubmedclip' # 'biomedclip' или 'pubmedclip'\n",
        "    MLP_OUTPUT_DIM = 4096\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    MODEL_PATHS = {\n",
        "        'biomedclip': 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224',\n",
        "        'pubmedclip': 'flaviagiammarino/pubmed-clip-vit-base-patch32'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- 4. Запускаю пайплайн: {ENCODER_CHOICE} на slake ---\")\n",
        "\n",
        "    if ENCODER_CHOICE == 'biomedclip':\n",
        "        print(\"--- Загрузка BiomedCLIP... ---\")\n",
        "        model_obj, _, image_processor = open_clip.create_model_and_transforms('ViT-B-16', pretrained='openai')\n",
        "        checkpoint_path = hf_hub_download(repo_id=MODEL_PATHS['biomedclip'], filename=\"open_clip_pytorch_model.bin\")\n",
        "        model_obj.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE), strict=False)\n",
        "        vision_encoder = model_obj.visual\n",
        "        encoder_output_dim = model_obj.visual.proj.shape[1]\n",
        "        print(f\"--- BiomedCLIP загружен. Фактическая размерность выхода: {encoder_output_dim} ---\")\n",
        "    else: # pubmedclip\n",
        "        print(\"--- Загрузка PubMedCLIP... ---\")\n",
        "        vision_encoder = CLIPVisionModel.from_pretrained(MODEL_PATHS['pubmedclip'])\n",
        "        image_processor = AutoImageProcessor.from_pretrained(MODEL_PATHS['pubmedclip'])\n",
        "        encoder_output_dim = vision_encoder.config.hidden_size\n",
        "        print(f\"--- PubMedCLIP загружен. Фактическая размерность выхода: {encoder_output_dim} ---\")\n",
        "\n",
        "    dataset = SlakeDataset(image_processor=image_processor)\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "    model = VisionMLP(\n",
        "        vision_encoder=vision_encoder,\n",
        "        encoder_output_dim=encoder_output_dim,\n",
        "        mlp_output_dim=MLP_OUTPUT_DIM\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    for param in model.vision_encoder.parameters(): param.requires_grad = False\n",
        "\n",
        "    print(\"\\n--- 5. Обрабатываю один батч... ---\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(dataloader))\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        image_embeddings = model(images)\n",
        "\n",
        "        print(f\"\\n- Форма входного батча: {images.shape}\")\n",
        "        print(f\"- Форма выходных эмбеддингов: {image_embeddings.shape}\")\n",
        "        assert image_embeddings.shape[1] == MLP_OUTPUT_DIM, \"Размерность эмбеддинга неверна!\"\n",
        "        print(\"\\n--- Проверка формы пройдена. ---\")\n",
        "\n",
        "    print(\"\\n\\n*** ПАЙПЛАЙН УСПЕШНО ВЫПОЛНЕН ***\")\n",
        "\n",
        "# --- 5. ЗАПУСК ---\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "rH3zIQ3Kg-2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "4c98e8756a2449ef9eb55696afe89227",
            "efb3b6e0dc0b420db274a99d6e9a4fe6",
            "da7b1486b4bf4b94ac87c326dafba268",
            "abfb6a4b3e2a425c901e71aed6823c35",
            "9b29086b9fee487b902839f24dfcc6ca",
            "5b21b97f59d144f78701e69ca069fd0e",
            "7e4f16beb91c4482ab7c8a4d72e7cab6",
            "9eda1d297b3a47a9898555aa77469003",
            "007fb49da02643a79afa59da4a37408d",
            "0aafbec6573a4e71832a85b36098099f",
            "6d3951f9a1524d539bfa1a0b1f73d20d"
          ]
        },
        "outputId": "763cef4c-764e-41dc-f9a6-bb8e28aed435"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Установка правильных зависимостей (включая open_clip_torch) ---\n",
            "\n",
            "--- 4. Запускаю пайплайн: pubmedclip на slake ---\n",
            "--- Загрузка PubMedCLIP... ---\n",
            "--- PubMedCLIP загружен. Фактическая размерность выхода: 768 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c98e8756a2449ef9eb55696afe89227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MLP сконфигурирован: 768 -> 4096 ---\n",
            "\n",
            "--- 5. Обрабатываю один батч... ---\n",
            "\n",
            "- Форма входного батча: torch.Size([4, 3, 224, 224])\n",
            "- Форма выходных эмбеддингов: torch.Size([4, 4096])\n",
            "\n",
            "--- Проверка формы пройдена. ---\n",
            "\n",
            "\n",
            "*** ПАЙПЛАЙН УСПЕШНО ВЫПОЛНЕН ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__gpbflmpWVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}