{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22ff2562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers datasets accelerate Pillow huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7374147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPVisionModel, AutoImageProcessor\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6bb3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATALOADER ДЛЯ VQA-RAD (ЧЕРЕЗ HF DATASETS) ---\n",
    "class VqaradDataset(Dataset):\n",
    "    def __init__(self, image_processor, split='train', hf_repo_id='flaviagiammarino/vqa-rad'):\n",
    "        self.image_processor = image_processor\n",
    "        self.dataset = load_dataset(hf_repo_id, split=split, streaming=False)\n",
    "        self.dataset = list(self.dataset)  # для небольшого датасета допустимо\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image'].convert(\"RGB\")\n",
    "        processed_output = self.image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "        if hasattr(processed_output, 'pixel_values'):\n",
    "            processed_image = processed_output.pixel_values\n",
    "        else:\n",
    "            processed_image = processed_output\n",
    "\n",
    "        if processed_image.dim() == 4:\n",
    "            processed_image = processed_image.squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"image\": processed_image,\n",
    "            \"question\": item['question'],\n",
    "            \"answer\": item['answer']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03a8526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATALOADER ДЛЯ SLAKE ---\n",
    "class SlakeDataset(Dataset):\n",
    "    def __init__(self, image_processor, split='train', hf_repo_id='BoKelvin/SLAKE'):\n",
    "        self.image_processor = image_processor\n",
    "        self.root_dir = snapshot_download(repo_id=hf_repo_id, repo_type='dataset')\n",
    "        img_dir_path = os.path.join(self.root_dir, 'imgs')\n",
    "        if not os.path.exists(img_dir_path):\n",
    "            os.system(f\"unzip -q -o {os.path.join(self.root_dir, 'imgs.zip')} -d {self.root_dir}\")\n",
    "        json_path = os.path.join(self.root_dir, f\"{split}.json\")\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            full_data = json.load(f)\n",
    "        self.dataset = [item for item in full_data if item['q_lang'] == 'en']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image_name = item['img_name']\n",
    "        image_name = image_name.split('/')\n",
    "        image_name = os.path.join(image_name[0], image_name[1])\n",
    "        image_path = os.path.join(self.root_dir, 'imgs', image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        processed_output = self.image_processor(image, return_tensors=\"pt\")\n",
    "        if hasattr(processed_output, 'pixel_values'):\n",
    "            processed_image = processed_output.pixel_values\n",
    "        else:\n",
    "            processed_image = processed_output\n",
    "\n",
    "        if processed_image.dim() == 4:\n",
    "            processed_image = processed_image.squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"image\": processed_image,\n",
    "            \"question\": item['question'],\n",
    "            \"answer\": item['answer']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b58694a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. КЛАСС МОДЕЛИ (encoder + MLP)---\n",
    "class VisionMLP(nn.Module):\n",
    "    def __init__(self, vision_encoder, encoder_output_dim, mlp_output_dim, hidden_dim=2560):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        \n",
    "        # MLP для обработки последовательности патчей\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(encoder_output_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, mlp_output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vision_encoder(pixel_values, output_hidden_states=False)\n",
    "        # Берём ВСЕ ПАТЧИ (без [CLS] токена)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:, :]  # (batch, num_patches, hidden_dim)\n",
    "        return self.mlp(patch_tokens)  # (batch, num_patches, mlp_output_dim)\n",
    "\n",
    "\n",
    "def get_model(encoder_choice, mlp_output_dim, hidden_dim=2560):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_paths = {\n",
    "        'standard_clip': 'openai/clip-vit-base-patch32',\n",
    "        'pubmedclip': 'flaviagiammarino/pubmed-clip-vit-base-patch32',\n",
    "        # Примеры других CLIP-моделей:\n",
    "        # 'biomed_clip': 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224',  # если есть HF версия\n",
    "        # 'large_clip': 'openai/clip-vit-large-patch14'\n",
    "    }\n",
    "\n",
    "    vision_encoder = CLIPVisionModel.from_pretrained(model_paths[encoder_choice])\n",
    "    image_processor = AutoImageProcessor.from_pretrained(model_paths[encoder_choice])\n",
    "    \n",
    "    # Получаем размерность скрытого состояния\n",
    "    encoder_output_dim = vision_encoder.config.hidden_size\n",
    "    \n",
    "\n",
    "    model = VisionMLP(\n",
    "        vision_encoder=vision_encoder,\n",
    "        encoder_output_dim=encoder_output_dim,\n",
    "        mlp_output_dim=mlp_output_dim\n",
    "    ).to(device)\n",
    "\n",
    "    # Замораживаем CLIP (только обучаем MLP)\n",
    "    for param in model.vision_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model, image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d312c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset_choice, image_processor, batch_size=4):\n",
    "    if dataset_choice == 'slake':\n",
    "        dataset = SlakeDataset(image_processor=image_processor, split='train')\n",
    "    elif dataset_choice == 'vqa_rad':\n",
    "        dataset = VqaradDataset(image_processor=image_processor, split='train')\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e0b00",
   "metadata": {},
   "source": [
    "### Дальше тест что все работает, по сути нужная часть - следующая (получение замороженного энкодера с размороженным MLP и даталоадера):\n",
    "\n",
    "    model, image_processor = get_model(encoder_choice=encoder_choice, mlp_output_dim=mlp_output_dim)\n",
    "    dataloader = get_dataloader(dataset_choice=dataset_choice, image_processor=image_processor, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77766588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(encoder_choice, dataset_choice, mlp_output_dim=4096):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(\"\\n--- Обработка батча с ПАТЧАМИ ---\")\n",
    "    model, image_processor = get_model(encoder_choice=encoder_choice, mlp_output_dim=mlp_output_dim)\n",
    "    dataloader = get_dataloader(dataset_choice=dataset_choice, image_processor=image_processor, batch_size=4)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dataloader))\n",
    "        images = batch['image'].to(device)\n",
    "        image_embeddings = model(images)\n",
    "\n",
    "        print(f\"\\n- Форма изображений: {images.shape}\")\n",
    "        print(f\"- Форма патч-эмбеддингов: {image_embeddings.shape}\")\n",
    "\n",
    "        assert image_embeddings.dim() == 3, \"Ожидалась 3D-форма для патчей!\"\n",
    "        assert image_embeddings.shape[-1] == mlp_output_dim, \"Неверная размерность эмбеддинга!\"\n",
    "        \n",
    "        expected_patches = (224 // 32) ** 2  # = 49\n",
    "\n",
    "        print(f\"- Патчей на изображение: {image_embeddings.shape[1]} (ожидалось ~{expected_patches})\")\n",
    "        print(image_embeddings[0][0])\n",
    "    \n",
    "    print(\"\\n*** УСПЕХ: Получены эмбеддинги ВСЕХ ПАТЧЕЙ ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff262378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Обработка батча с ПАТЧАМИ ---\n",
      "\n",
      "- Форма изображений: torch.Size([4, 3, 224, 224])\n",
      "- Форма патч-эмбеддингов: torch.Size([4, 49, 4096])\n",
      "- Патчей на изображение: 49 (ожидалось ~49)\n",
      "tensor([-0.0026,  0.0096,  0.0352,  ...,  0.0510,  0.1204, -0.0607],\n",
      "       device='cuda:0')\n",
      "\n",
      "*** УСПЕХ: Получены эмбеддинги ВСЕХ ПАТЧЕЙ ***\n"
     ]
    }
   ],
   "source": [
    "eval(encoder_choice='pubmedclip', dataset_choice='vqa_rad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a5544e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Обработка батча с ПАТЧАМИ ---\n",
      "\n",
      "- Форма изображений: torch.Size([4, 3, 224, 224])\n",
      "- Форма патч-эмбеддингов: torch.Size([4, 49, 4096])\n",
      "- Патчей на изображение: 49 (ожидалось ~49)\n",
      "tensor([-0.0485, -0.0370,  0.0320,  ...,  0.2097,  0.1049,  0.1029],\n",
      "       device='cuda:0')\n",
      "\n",
      "*** УСПЕХ: Получены эмбеддинги ВСЕХ ПАТЧЕЙ ***\n"
     ]
    }
   ],
   "source": [
    "eval(encoder_choice='standard_clip', dataset_choice='vqa_rad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93ea8c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Обработка батча с ПАТЧАМИ ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edf00e4aaec4269a2dfa1a82ac53182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Форма изображений: torch.Size([4, 3, 224, 224])\n",
      "- Форма патч-эмбеддингов: torch.Size([4, 49, 4096])\n",
      "- Патчей на изображение: 49 (ожидалось ~49)\n",
      "tensor([-0.0887,  0.0570, -0.0225,  ..., -0.1287,  0.0247,  0.2395],\n",
      "       device='cuda:0')\n",
      "\n",
      "*** УСПЕХ: Получены эмбеддинги ВСЕХ ПАТЧЕЙ ***\n"
     ]
    }
   ],
   "source": [
    "eval(encoder_choice='pubmedclip', dataset_choice='slake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67045035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Обработка батча с ПАТЧАМИ ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9d72228ec1432f8d94cce7290faee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Форма изображений: torch.Size([4, 3, 224, 224])\n",
      "- Форма патч-эмбеддингов: torch.Size([4, 49, 4096])\n",
      "- Патчей на изображение: 49 (ожидалось ~49)\n",
      "tensor([-0.0504, -0.1674, -0.0298,  ..., -0.0098, -0.1319,  0.0947],\n",
      "       device='cuda:0')\n",
      "\n",
      "*** УСПЕХ: Получены эмбеддинги ВСЕХ ПАТЧЕЙ ***\n"
     ]
    }
   ],
   "source": [
    "eval(encoder_choice='standard_clip', dataset_choice='slake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805714d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d155c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
