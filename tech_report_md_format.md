Отчет о ходе реализации индустриального проекта
Тема: «Medical Image Q&A Bot»

1. Введение и постановка проблемы

Проект «Medical Image Q&A Bot» направлен на создание интеллектуального ассистента для анализа медицинских изображений.

Решаемая проблема: Существует значительный информационный разрыв между медицинскими специалистами и пациентами. Пациенты, получая на руки результаты рентгена, КТ или МРТ, часто не могут самостоятельно интерпретировать заключение или изображение. В то же время, врачи перегружены работой и не всегда имеют возможность отвечать на базовые вопросы пациентов. Это создает тревожность у пациентов и увеличивает нагрузку на систему здравоохранения.

Целевая аудитория:

Пациенты: для получения первичных, понятных разъяснений по своим медицинским снимкам.

Студенты медицинских вузов: в качестве образовательного инструмента для тренировки навыков интерпретации изображений.

Медицинские работники: для получения «второго мнения» или быстрой проверки по простым кейсам.

Цель проекта: Разработать систему Visual Question Answering (VQA), способную давать краткие и точные ответы на вопросы по медицинским изображениям, и интегрировать ее в удобный интерфейс (Telegram-бот).

2. Работа с данными: Выбор и подготовка

Для обучения и валидации модели были выбраны два ключевых датасета, наиболее релевантных поставленной задаче.

2.1. Выбор датасетов:

SLAKE (Semantically-Labeled Knowledge-Enhanced): Выбран как основной датасет благодаря своему разнообразию медицинских модальностей (КТ, МРТ, Рентген) и большому количеству вопросов-ответов.

VQA-RAD (Radiology VQA): Выбран в качестве дополнительного датасета. Его ценность заключается в том, что вопросы и ответы составлены непосредственно радиологами, что обеспечивает высокую клиническую релевантность.

Обоснование выбора: Оба датасета содержат большое количество вопросов, требующих коротких и фактических ответов (например, "Is there evidence of cancer?", "Which organ is shown?"). Такая структура идеально соответствует нашей основной цели — предоставлять быстрые и понятные ответы для целевой аудитории, разгружая врачей от рутинных вопросов.

2.2. Предобработка данных:

Фильтрация данных: Исходный датасет SLAKE содержал вопросы на нескольких языках, включая китайский. Для обеспечения консистентности модели была проведена фильтрация, в результате которой в обучающей и тестовой выборках остались только англоязычные пары "вопрос-ответ".

Разработка Dataloader'ов: Для каждого датасета были написаны кастомные классы-загрузчики. Они выполняют всю необходимую подготовительную работу: корректно загружают изображения по путям, указанным в JSON-файлах, сопоставляют их с соответствующими текстовыми данными (вопрос, ответ) и формируют батчи для подачи в модель.

3. Архитектура и протестированные модели

Основой нашего решения является современная мультимодальная архитектура, состоящая из трех ключевых компонентов:

Vision Encoder (Кодировщик изображений) -> Projector (Проекционный слой) -> LLM (Большая языковая модель)

3.1. Протестированные архитектуры:

Было реализовано и протестировано два варианта архитектуры, отличающихся кодировщиком изображений:

CLIP-ViT + MLP + Qwen: Использование стандартного клип энкодера как бейзлайн для сравнения со специализированными энкодерами.

PubMedCLIP + MLP + Qwen: Альтернативная архитектура с использованием другого популярного медицинского Vision-энкодера.

3.2. Компоненты:

Vision Encoders: В качестве кодировщиков были выбраны openai/clip-vit-base-patch32 и flaviagiammarino/pubmed-clip-vit-base-patch32. Их выбор обусловлен тем, что они предобучены на огромном корпусе медицинских данных, что позволяет им эффективно извлекать релевантные признаки из снимков.

Проекционный слой (Projector): В качестве проектора используется простой, но эффективный многослойный перцептрон (MLP). Его задача — преобразовать векторное представление изображения (эмбеддинг), полученное от Vision Encoder, в пространство признаков, понятное для большой языковой модели (Qwen).

4. Стратегия обучения и текущий статус

Для эффективного обучения была выбрана двухэтапная стратегия, аналогичная подходу LLaVA.

Этап 1 (Завершен): Обучение проекционного слоя. На данном этапе были «заморожены» веса Vision Encoder и LLM, и производилось обучение только MLP-проектора. Это позволило быстро и с минимальными ресурсами научить модель сопоставлять визуальные признаки с текстовым пространством.

Этап 2 (Планируется): Fine-tuning языковой модели. На следующем этапе планируется заморозить веса Vision Encoder и обученного MLP, и дообучать большую языковую модель (Qwen) с использованием эффективной техники LoRA (Low-Rank Adaptation). Это позволит LLM адаптироваться к специфике медицинских вопросов и генерировать корректные ответы.

5. Дальнейшие шаги и интерфейс

Интерфейс и демонстрация: Для быстрой демонстрации возможностей и итеративного тестирования модели ведется разработка интерактивного веб-интерфейса с использованием Gradio.

Архитектурные улучшения (на стадии исследования): Рассматривается замена MLP-проектора на более сложный механизм Cross-Attention. Это потенциально позволит модели более детально и контекстуально связывать конкретные части вопроса с конкретными областями на изображении, повышая точность и релевантность ответов.

6. Заключение

На текущий момент успешно завершен этап подготовки данных и построения базовой архитектуры. Написаны и отлажены даталоадеры, протестированы два state-of-the-art Vision-энкодера и проведено обучение проекционного слоя. Проект находится на стадии готовности к основному этапу — дообучению языковой модели Qwen с помощью LoRA, после чего начнется работа над интеграцией в пользовательские интерфейсы.






